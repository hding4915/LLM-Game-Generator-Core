import openai
import requests
import json
from config import config
from src.generation.arcade_tools import get_arcade_3_0_api_conventions, search_arcade_kb
from typing import List, Dict, Any, Optional

from src.rag_service.rag import RagService


def get_client_config(provider: str) -> dict | None:
    """
    æ ¹æ“š Provider å›å‚³å°æ‡‰çš„ Client è¨­å®š (api_key, base_url)
    æ”¯æ´ OpenAI ç›¸å®¹ä»‹é¢çš„æœå‹™ (Groq, Mistral, DeepSeek)
    æ³¨æ„ï¼šOllama å·²ç¨ç«‹è™•ç†ï¼Œä¸åœ¨æ­¤å‡½å¼ä¸­
    """
    provider = provider.lower()

    if provider == "openai":
        return {
            "api_key": config.OPENAI_API_KEY,
            "base_url": None
        }
    elif provider == "groq":
        return {
            "api_key": config.GROQ_API_KEY,
            "base_url": "https://api.groq.com/openai/v1"
        }
    elif provider == "mistral":
        return {
            "api_key": config.MISTRAL_API_KEY,
            "base_url": "https://api.mistral.ai/v1"
        }
    elif provider == "deepseek":
        return {
            "api_key": config.DEEPSEEK_API_KEY,
            "base_url": "https://api.deepseek.com/v1"
        }
    elif provider == "inception":
        return {
            "api_key": config.INCEPTION_API_KEY,
            "base_url": "https://api.inceptionlabs.ai/v1"
        }
    return None


def call_google_gemini(
        system_prompt: str,
        user_prompt: str,
        model: str,
        temperature: float,
        max_tokens: int = 8192
) -> str:
    """
    è™•ç† Google Gemini çš„ç‰¹æ®Šé‚è¼¯ (éœ€å®‰è£ google-generativeai)
    """
    try:
        import google.generativeai as genai
    except ImportError:
        return "Error: è«‹å®‰è£ google-generativeai å¥—ä»¶ (pip install google-generativeai)"

    api_key: str = config.GOOGLE_API_KEY
    if not api_key:
        return "Error: æœªè¨­å®š GOOGLE_API_KEY"

    try:
        genai.configure(api_key=api_key)

        generation_config: dict = {
            "temperature": temperature,
            "top_p": 0.95,
            "max_output_tokens": max_tokens,
            "response_mime_type": "text/plain",
        }

        # System instructions
        gemini_model = genai.GenerativeModel(
            model_name=model,
            generation_config=generation_config,
            system_instruction=system_prompt
        )

        response = gemini_model.generate_content(user_prompt)
        return response.text
    except Exception as e:
        return f"Gemini API Error: {str(e)}"


def call_ollama(
        system_prompt: str,
        user_prompt: str,
        model: str,
        temperature: float,
        num_ctx: int = 4096
) -> str:

    print(f"Run ollama (Native API): {model}")

    base_url = config.OLLAMA_BASE_URL
    if not base_url:
        base_url = "http://localhost:11434"

    # æ¸…ç† URLï¼Œç¢ºä¿æŒ‡å‘ /api/chat
    api_url = base_url.rstrip("/")
    # å¦‚æœåŸæœ¬è¨­å®šåŒ…å« /v1 (ç‚ºäº†ç›¸å®¹ OpenAI)ï¼Œè¦æŠŠå®ƒæ‹¿æ‰æ”¹æˆåŸç”Ÿè·¯å¾‘
    if api_url.endswith("/v1"):
        api_url = api_url[:-3]
    api_url = f"{api_url}/api/chat"

    # è¨­å®š Request Body
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "stream": False,
        "options": {
            "num_ctx": num_ctx,
            "temperature": temperature
        }
    }

    headers = {
        "Content-Type": "application/json"
    }

    if config.OLLAMA_API_KEY:
        headers["Authorization"] = f"Bearer {config.OLLAMA_API_KEY}"

    response = ""


    try:
        response = requests.post(
            api_url,
            json=payload,
            headers=headers,  # å¸¶ä¸Š headers
            timeout=300
        )

        # æª¢æŸ¥æ˜¯å¦æœ‰ 401 (Unauthorized) æˆ– 403 (Forbidden) ç­‰éŒ¯èª¤
        if response.status_code == 401:
            return "Ollama Error: 401 Unauthorized. è«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢ºã€‚"

        response.raise_for_status()

        result = response.json()
        return result["message"]["content"]

    except requests.exceptions.RequestException as e:
        print(f"[Ollama Error] Connection failed: {e}")
        return f"Ollama Error: {str(e)}"
    except KeyError:
        return f"Ollama Error: Unexpected response format. {response.text}"


def execute_tool(tool_name: str, args: dict, rag_instance: Any = None) -> str:
    """
    æ ¹æ“šå·¥å…·åç¨±åŸ·è¡Œå°æ‡‰çš„æœ¬åœ°å‡½æ•¸ã€‚
    """
    try:
        from src.generation.arcade_tools import get_arcade_3_0_api_conventions, search_arcade_kb
    except ImportError:
        return f"Error: Could not import game_generator tools. Check project structure."

    if tool_name == "get_arcade_3_0_api_conventions":
        return get_arcade_3_0_api_conventions()

    if tool_name == "search_arcade_kb":
        query = args.get("query", "")
        if not rag_instance:
            return "Error: RAG instance is not initialized or passed correctly."
        return search_arcade_kb(query=query, rag=rag_instance)

    return f"Error: Tool '{tool_name}' not found."


def call_llm(
        system_prompt: str,
        user_prompt: str,
        provider: str = "openai",
        model: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: int = 8192,
        tools: Optional[List[Dict[str, Any]]] = None,
        rag_instance: Any = None,
        tool_additional_instruction: str = None  # [æ–°å¢åƒæ•¸] å…è¨±å¤–éƒ¨æ³¨å…¥ç‰¹å®šçš„æé†’
) -> str:
    """
    [çµ±ä¸€å…¥å£] æ”¯æ´å¤šç¨® LLM Provider ä¸¦æ•´åˆ Tool Use è¿´åœˆã€‚
    """
    provider = provider.lower()

    if provider in ["google", "gemini"]:
        if model.startswith("gpt"):
            model = "gemini-2.5-flash-preview-09-2025"
        try:
            from src.utils.llm_clients import call_google_gemini
            return call_google_gemini(system_prompt, user_prompt, model, temperature, max_tokens=max_tokens)
        except ImportError:
            return "Error: call_google_gemini not found."

    if provider == "ollama":
        try:
            from src.utils.llm_clients import call_ollama
            return call_ollama(system_prompt, user_prompt, model, temperature, num_ctx=8192)
        except ImportError:
            return "Error: call_ollama not found."

    openai_config = get_client_config(provider)
    if not openai_config:
        return f"Error: ä¸æ”¯æ´çš„ Provider '{provider}'"

    api_key = openai_config.get("api_key")
    base_url = openai_config.get("base_url")

    try:
        client = openai.OpenAI(api_key=api_key, base_url=base_url)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # Tool Loop: æœ€å¤šå…è¨± 5 æ¬¡å¾€è¿”
        for loop_index in range(5):
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "timeout": 600
            }

            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"

            response = client.chat.completions.create(**kwargs)
            assistant_message = response.choices[0].message

            # 1. æª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·å‘¼å«
            if not assistant_message.tool_calls:
                return assistant_message.content if assistant_message.content else ""

            # 2. è™•ç†å·¥å…·å‘¼å«
            tool_calls_list = []
            for tc in assistant_message.tool_calls:
                tool_calls_list.append({
                    "id": tc.id,
                    "type": "function",
                    "function": {
                        "name": tc.function.name,
                        "arguments": tc.function.arguments
                    }
                })

            messages.append({
                "role": "assistant",
                "content": assistant_message.content or "",
                "tool_calls": tool_calls_list
            })

            # 3. åŸ·è¡Œæ‰€æœ‰å·¥å…·
            for tc in tool_calls_list:
                function_name = tc["function"]["name"]
                try:
                    function_args = json.loads(tc["function"]["arguments"])
                except json.JSONDecodeError:
                    function_args = {}

                print(f"ğŸ› ï¸ [Tool Call] åŸ·è¡Œå·¥å…·: {function_name} | åƒæ•¸: {function_args}")
                observation = execute_tool(function_name, function_args, rag_instance=rag_instance)
                print(f"   -> Result: {observation[:200]}..." if observation else "   -> Result: (Empty)")

                messages.append({
                    "tool_call_id": tc["id"],
                    "role": "tool",
                    "name": function_name,
                    "content": observation
                })

            # [Nudge Logic] ä½¿ç”¨å‚³å…¥çš„åƒæ•¸ï¼Œå¦‚æœæ²’å‚³å‰‡ä½¿ç”¨é€šç”¨æé†’
            default_instruction = (
                "Tool outputs provided above. "
                "Please generate the code now based on these findings."
            )

            # å„ªå…ˆä½¿ç”¨å¤–éƒ¨å‚³å…¥çš„æŒ‡ä»¤ï¼Œå¦å‰‡ä½¿ç”¨é è¨­
            final_instruction = tool_additional_instruction if tool_additional_instruction else default_instruction

            messages.append({
                "role": "user",
                "content": final_instruction
            })

    except Exception as e:
        print(f"[LLM Call Error] Provider: {provider}, Error: {e}")
        return f"LLM Call Error ({provider}): {str(e)}"

    return "Error: Tool loop exceeded limit."